FROM gettyimages/spark:2.4.0-hadoop-3.0

# Required for pyspark installation in requirements
RUN pip install wheel==0.34.2

ADD ./conf/jars/* /usr/local/spark/jars/

ADD ./requirements.txt /tmp/requirements.txt
RUN pip install -r /tmp/requirements.txt

# Download necessary data for nltk
RUN python -c "import nltk; nltk.download('stopwords'); nltk.download('punkt')"
